{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "warnings.filterwarnings('ignore')\n",
    "#train.drop(['education'], axis=1,inplace=True)\n",
    "#train.drop(['relationship'], axis=1,inplace=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = train['income']\n",
    "train.drop(['income'], axis=1, inplace=True)\n",
    "\n",
    "train = pd.get_dummies(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8569047 , 0.84912476, 0.85218116, 0.854404  , 0.85579328,\n",
       "       0.86190609, 0.85495971, 0.84745763, 0.86079467, 0.85376703])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100,max_features='sqrt',max_depth=None,min_samples_split=2,bootstrap=True)\n",
    "rf_scores = cross_val_score(rf_clf,train,y_label,cv=10)\n",
    "rf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.fit(train,y_label)\n",
    "importances=rf_clf.feature_importances_\n",
    "names=train.columns.values.tolist()\n",
    "sort_lst=sorted(zip(map(lambda x: round(x,3),importances),names), reverse=True)\n",
    "#importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = pd.read_csv(\"train.csv\")\n",
    "train_new.drop(['income'], axis=1,inplace=True)\n",
    "\n",
    "new_country = []\n",
    "for item in list(train_new['native-country']):\n",
    "    if item != 'United-States':\n",
    "        new_country.append('Others')\n",
    "    else:\n",
    "        new_country.append(item)\n",
    "        \n",
    "\n",
    "train_new['native-country'] = new_country\n",
    "train_new = pd.get_dummies(train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "def diff(pred, x_test):\n",
    "    gender = list(x_test)\n",
    "    g0_actual = [i for i,label in enumerate(gender) if label==0]\n",
    "    g1_actual = [i for i,label in enumerate(gender) if label==1]\n",
    "    pred_1 = [i for i,label in enumerate(pred) if label==1]\n",
    "    \n",
    "    diff = abs((len(set(g0_actual)&set(pred_1))/len(g0_actual))-(len(set(g1_actual)&set(pred_1))/len(g1_actual)))\n",
    "    return diff\n",
    "\n",
    "def score(diff, accuracy):\n",
    "    if diff<=0.1:\n",
    "        return accuracy\n",
    "    else:\n",
    "        score=accuracy-7**(diff-0.1)+1\n",
    "        return score\n",
    "\n",
    "def loss_function(y,y_pred,greater_is_better=True):\n",
    "    acc=(y==y_pred).mean()\n",
    "    diff_=diff(y,train['gender'])\n",
    "    return score(diff_,acc)\n",
    "\n",
    "my_scorer=make_scorer(loss_function,greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8000444592641992"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "best_score_lr=0\n",
    "for c in [0.001,0.01,0.1,1,10,50]:\n",
    "    for weight in [None,'balanced']:\n",
    "        fair_=[]\n",
    "        time=0\n",
    "        while (time<5):\n",
    "            x_train,x_test,y_train,y_test = train_test_split(train_new,y_label,test_size=0.25,random_state=None)\n",
    "            lr = LogisticRegression(penalty='l2',class_weight=weight,C=c).fit(x_train,y_train)\n",
    "            acc=lr.score(x_test,y_test)\n",
    "            pred = list(lr.predict(x_test))\n",
    "            gender = list(x_test['gender'])\n",
    "            g0_act = [i for i,label in enumerate(gender) if label==0]\n",
    "            g1_act = [i for i,label in enumerate(gender) if label==1]\n",
    "            label_1_pred = [i for i,label in enumerate(pred) if label==1]\n",
    "            \n",
    "            diff=abs((len(set(g0_act)&set(label_1_pred))/len(g0_act))-(len(set(g1_act)&set(label_1_pred))/len(g1_act)))\n",
    "            if diff <= 0.1:\n",
    "                fair_.append(acc)\n",
    "            else:\n",
    "                acc = acc-7**(diff-0.1)+1\n",
    "                fair_.append(acc)\n",
    "            time += 1\n",
    "            \n",
    "        mean = np.mean(fair_)\n",
    "        if mean > best_score_lr:\n",
    "            best_score_lr = mean\n",
    "            best_param_lr = {'class_weight':weight,'C':c}\n",
    "\n",
    "best_score_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7535037996958146"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "best_score_xgb = 0\n",
    "for est in [50,100,500]:\n",
    "    for lr in [0.0005,0.001,0.005,0.1,1]:\n",
    "        fair_=[]\n",
    "        time=0\n",
    "        while (time<5):\n",
    "            x_train,x_test,y_train,y_test = train_test_split(train_new,y_label,test_size=0.25,random_state=None)\n",
    "            xgb_lr = xgb.XGBClassifier(learning_rate=lr, n_estimators=est).fit(x_train,y_train)\n",
    "            acc=xgb_lr.score(x_test,y_test)\n",
    "            pred = list(xgb_lr.predict(x_test))\n",
    "            gender = list(x_test['gender'])\n",
    "            g0_act = [i for i,label in enumerate(gender) if label==0]\n",
    "            g1_act = [i for i,label in enumerate(gender) if label==1]\n",
    "            label_1_pred = [i for i,label in enumerate(pred) if label==1]\n",
    "            \n",
    "            diff=abs((len(set(g0_act)&set(label_1_pred))/len(g0_act))-(len(set(g1_act)&set(label_1_pred))/len(g1_act)))\n",
    "            if diff <= 0.1:\n",
    "                fair_.append(acc)\n",
    "            else:\n",
    "                acc = acc-7**(diff-0.1)+1\n",
    "                fair_.append(acc)\n",
    "            time += 1\n",
    "            \n",
    "        mean = np.mean(fair_)\n",
    "        if mean > best_score_xgb:\n",
    "            best_score_xgb = mean\n",
    "            best_param_xgb = {'n_estimators':est, 'learning_rate':lr}\n",
    "            \n",
    "best_score_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7020387847207541"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "best_score_rf = 0\n",
    "for est in [50,100,200,500,1000]:\n",
    "    for fea in np.arange(4,22,4):\n",
    "        fair_=[]\n",
    "        time=0\n",
    "        while (time<5):\n",
    "            x_train,x_test,y_train,y_test = train_test_split(train_new,y_label,test_size=0.25,random_state=None)\n",
    "            rf = RandomForestClassifier(n_estimators=est,max_features=fea,max_depth=None,min_samples_split=2,bootstrap=True).fit(x_train,y_train)\n",
    "            acc = rf.score(x_test, y_test)\n",
    "            pred = list(rf.predict(x_test))\n",
    "            gender = list(x_test['gender'])\n",
    "            g0_act = [i for i,label in enumerate(gender) if label==0]\n",
    "            g1_act = [i for i,label in enumerate(gender) if label==1]\n",
    "            label_1_pred = [i for i,label in enumerate(pred) if label==1]\n",
    "\n",
    "            diff = abs((len(set(g0_act)&set(label_1_pred))/len(g0_act))-(len(set(g1_act)&set(label_1_pred))/len(g1_act)))\n",
    "            if diff <= 0.1:\n",
    "                fair_.append(acc)\n",
    "            else:\n",
    "                acc = acc-7**(diff-0.1)+1\n",
    "                fair_.append(acc)\n",
    "            time += 1\n",
    "        \n",
    "        mean = np.mean(fair_)\n",
    "        if mean > best_score_rf:\n",
    "            best_score_rf = mean\n",
    "            best_param_rf = {'n_estimators':est, 'max_features':fea}\n",
    "            \n",
    "best_score_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that for XGBoost, the optimal model has an overall score of 0.75 (almost has DDP value of 0.17), while Random Forest only has 0.70 at most. In terms of Logistic Regression classifier, it has the accuracy of 0.80 but a lower bias (DDP value is only 0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VoteClassifier to combine LR & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222463043236634"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(learning_rate=best_param_xgb['learning_rate'], \n",
    "                              n_estimators=best_param_xgb['n_estimators'])\n",
    "\n",
    "lr_model = LogisticRegression(penalty='l2',class_weight=best_param_lr['class_weight'],\n",
    "                              C=best_param_lr['C'])\n",
    "\n",
    "estimators=[('rf', lr_model), ('xgb', xgb_model)]\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "ensemble.fit(train_new, y_label)\n",
    "\n",
    "loss_function(ensemble.predict(train_new), y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8358351001460346"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Two XGBoost models and one LR model get a best loss_function_score, which is 0.836\n",
    "estimators_2=[('rf', lr_model), ('xgb', xgb_model),('xgb_2', xgb_model)]\n",
    "ensemble_2 = VotingClassifier(estimators_2, voting='soft')\n",
    "ensemble_2.fit(train_new, y_label)\n",
    "\n",
    "loss_function(ensemble_2.predict(train_new), y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further improvement, we would import voting classifier, which includes both XGBoost and Logistic Regression. That means we could achieve higher accuracy by just using Logistic Regression or lower bias by just using XGBoost. Comparing two ways of handling features, we used two XGBoost models and one Logistic Regression model to do the voting, causing a result of 0.836 accuracy and 0.095 DDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
